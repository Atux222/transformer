import torch
from transformer_first import Transformer
from transformers import AutoTokenizer
from datasets import load_from_disk
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

# === åŠ è½½æ¨¡å‹ä¸ Tokenizer ===
checkpoint_file = "/root/xyy/checkpoint_epoch_8.pth"
model_path = "/root/xyy/token_model"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Transformer é…ç½®      
d_model = 512
d_ff = 2048
d_k = d_v = 64
n_heads = 8
n_layers = 6
drop_prob = 0.1
max_length = 128
batch_size = 16

# åŠ è½½ Tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path)

model = Transformer(
    src_pad_idx=tokenizer.pad_token_id,
    trg_pad_idx=tokenizer.pad_token_id,
    enc_voc_size=tokenizer.vocab_size,
    dec_voc_size=tokenizer.vocab_size,
    d_model=d_model,
    max_len=max_length,
    n_head=n_heads,
    ffn_hidden=d_ff,
    n_layer=n_layers,
    drop_prob=drop_prob,
    device=device,
).to(device)

# åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
checkpoint = torch.load(checkpoint_file, map_location=device)
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

# === ç¿»è¯‘å‡½æ•°ï¼Œè¿”å› token åˆ—è¡¨ ===
def translate_to_tokens(sentence, tokenizer, model, device, max_len=128):
    model.eval()
    with torch.no_grad():
        inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=max_len)
        src = inputs["input_ids"].to(device)

        trg_indices = [tokenizer.cls_token_id]
        for _ in range(max_len):
            trg_tensor = torch.tensor([trg_indices], dtype=torch.long).to(device)
            output = model(src, trg_tensor)
            next_token = output[0, -1, :].argmax(dim=-1).item()
            trg_indices.append(next_token)
            if next_token == tokenizer.sep_token_id:
                break

        # è¿”å› tokensï¼ˆå»é™¤ BOS å’Œ EOSï¼‰
        return tokenizer.convert_ids_to_tokens(trg_indices[1:-1])

# === åŠ è½½æµ‹è¯•é›† ===
test_dataset = load_from_disk("test_dataset_processed")

# === è®¡ç®— BLEU åˆ†æ•° ===
all_preds = []
all_targets = []
smoother = SmoothingFunction()

for example in tqdm(test_dataset, desc="ç¿»è¯‘ä¸­", unit="å¥å­"):
    src_text = example["translation"]["en"]
    ref_text = example["translation"]["de"]

    # æ¨¡å‹è¾“å‡º token åºåˆ—
    pred_tokens = translate_to_tokens(src_text, tokenizer, model, device=device)

    # å‚è€ƒç¿»è¯‘ token åºåˆ—
    ref_ids = tokenizer(ref_text, return_tensors="pt", truncation=True, max_length=max_length)["input_ids"][0]
    ref_tokens = tokenizer.convert_ids_to_tokens(ref_ids[1:-1])  # å»æ‰ BOS å’Œ EOS

    all_preds.append(pred_tokens)
    all_targets.append([ref_tokens])  # æ³¨æ„ï¼šå‚è€ƒç­”æ¡ˆæ˜¯äºŒç»´ç»“æ„

# === ç”¨ NLTK çš„ corpus_bleu è®¡ç®— BLEU-4 åˆ†æ•° ===
bleu_score = corpus_bleu(all_targets, all_preds,
                         smoothing_function=smoother.method1,
                         weights=(0.25, 0.25, 0.25, 0.25))

print(f"\nâœ… æµ‹è¯•é›† BLEU Scoreï¼ˆnltk versionï¼‰: {bleu_score:.4f}")

# === æ‰“å°ç¿»è¯‘æ ·ä¾‹ ===
print("\nğŸŒ ç¿»è¯‘ç¤ºä¾‹ï¼ˆå‰ 5 ä¸ªï¼‰:")
for i in range(min(5, len(all_preds))):
    pred_sentence = tokenizer.convert_tokens_to_string(all_preds[i])
    target_sentence = tokenizer.convert_tokens_to_string(all_targets[i][0])
    print(f"\nğŸ”¹ åŸæ–‡ï¼š{test_dataset[i]['translation']['en']}")
    print(f"ğŸ”¸ å‚è€ƒç¿»è¯‘ï¼š{target_sentence}")
    print(f"ğŸ”¸ æ¨¡å‹è¾“å‡ºï¼š{pred_sentence}")
